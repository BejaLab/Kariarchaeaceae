import sqlite3
from os import listdir, path
from pandas import read_excel, read_csv, read_sql_query
import yaml
import warnings
from math import ceil

# Rhodopsin sets (references and archaeal rhodopsins)
rhodopsin_sets ,= glob_wildcards("rhodopsins/{rhodopsin_set}.faa")

# Read metadata - genomes and rhodopsins
with warnings.catch_warnings():
    warnings.simplefilter("ignore")
    metadata = read_excel("metadata/genomes.xlsx")
    rhodopsins = read_excel("metadata/rhodopsins.xlsx")

# Set the species name
species = "Kariarchaeum pelagium"
genus = species.split()[0]

# Different slices of metadata
nonred_all = metadata[metadata['redundant'].isna()]
nonred_bins = nonred_all[nonred_all['is_not_bin'].isna()]

all_genomes = nonred_all['genome'].tolist()
species_genomes = nonred_bins[nonred_bins['classification'].str.contains(species)]['genome'].tolist()
other_genomes = nonred_bins[~nonred_bins['classification'].str.contains(species)]['genome'].tolist()
scaffold_to_pplace = nonred_all[~nonred_all['is_not_bin'].isna()].iloc[0]['genome']

# Read logan metadata and select datasets we are interested in
logan_file = 'metadata/logan_marine_datasets.csv'
if not path.exists(logan_file):
    logan_cnn = sqlite3.connect('databases/logan/metadata.sq3')
    with open('workflow/resources/logan_marine.sql') as file:
        logan_query = file.read()
    read_sql_query(logan_query, logan_cnn).set_index('accession').to_csv(logan_file)
logan_all = read_csv(logan_file)
logan_mg = logan_all.query("is_transcriptome == 0")

configfile: "workflow/config.yaml"
include: "Pangenome.snakefile"
include: "Species_phylogeny.snakefile"
include: "Proteins.snakefile"
include: "Profiling.snakefile"

# All of the final output files we need
rule all:
    input:
        "output/Variation_across_pangenome.svg",
        "output/Variation_across_pangenome.csv",
        "output/Variation_across_scaffold.svg",
        "output/Variation_across_scaffold.csv",
        "output/Distribution_of_Kariarchaeum_map.svg",
        "output/Distribution_of_Kariarchaeum_depths.svg",
        "output/Distribution_of_Kariarchaeum_data.csv",
        "output/Distribution_of_HeidallRs_relative_to_archaeal_proton_pumps.svg",
        "output/Distribution_of_HeidallRs_relative_to_archaeal_proton_pumps_combined.csv",
        "output/Distribution_of_HeidallRs_relative_to_archaeal_proton_pumps_OM-RGC2.csv",
        "output/Distribution_of_HeidallRs_relative_to_archaeal_proton_pumps_JGI.IMG.csv",
        "output/Distribution_of_HeidallRs_relative_to_all_rhodopsins.svg",
        "output/Distribution_of_HeidallRs_relative_to_all_rhodopsins_combined.csv",
        "output/Distribution_of_HeidallRs_relative_to_all_rhodopsins_OM-RGC2.csv",
        "output/Distribution_of_HeidallRs_relative_to_all_rhodopsins_JGI.IMG.csv",
        "output/Rhodopsins_NeighborNet_network.svg",
        "output/Species_phylogeny.svg",
        "output/Superpang_pangenome_annotated.gbk",
        "output/Rhodopsins_NeighborNet_network.nex"

# Copy genome for analysis
rule copy_genome:
    input:
        "genomes/{genome}.fna"
    output:
        "analysis/genomes/{genome}.fna"
    shell:
        "cp {input} {output}"

# Create fasta .fai index
rule faidx:
    input:
        "{prefix}"
    output:
        "{prefix}.fai"
    conda:
        "envs/kits.yaml"
    shell:
        "seqkit faidx {input}"

# Make protein blast database from fasta
rule makeblast_prot:
    input:
        "{prefix}.faa"
    output:
        "{prefix}.faa.pdb"
    conda:
        "envs/blast.yaml"
    shell:
        "makeblastdb -in {input} -dbtype prot"

# Make nucleotide blast database from fasta
rule makeblast_nucl:
    input:
        "{prefix}.fna"
    output:
        "{prefix}.fna.ndb"
    conda:
        "envs/blast.yaml"
    shell:
        "makeblastdb -in {input} -dbtype nucl"

# Concatenate all genomes
rule cat_genomes:
    input:
        expand("analysis/genomes/{genome}.faa", genome = all_genomes)
    output:
        "analysis/genomes/all_genomes.faa"
    shell:
        "cat {input} > {output}"

# Create fasta .dict index
rule fasta_dict:
    input:
        "{prefix}.fna"
    output:
        "{prefix}.dict"
    conda:
        "envs/gatk.yaml"
    shell:
        "gatk CreateSequenceDictionary -R {input} -O {output}"

# Create bam .bai index
rule bam_index:
    input:
        "{prefix}.bam"
    output:
        "{prefix}.bam.bai"
    conda:
        "envs/samtools.yaml"
    shell:
        "samtools index {input}"

# Convert fai index to intervals (list of sequences with their sizes)
rule fai_to_intervals:
    input:
        "{prefix}.fai"
    output:
        "{prefix}.fai.txt"
    shell:
        "awk '{{print$1,1,$2}}' OFS=\\\\t {input} > {output}"
